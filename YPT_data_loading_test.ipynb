{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, '10.1')\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('/raid-dgx1/Hasnat/Covoiturage/')\n",
    "from my_utilities_display_related import show_image, show_image_from_file, show_image_subplots\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print((torch.cuda.is_available(), torch.version.cuda)) \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Yolo_dataset\n",
    "from tool.tv_reference.utils import collate_fn as val_collate\n",
    "\n",
    "from cfg import Cfg\n",
    "import argparse\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "def get_args(**kwargs):\n",
    "    cfg = kwargs\n",
    "    parser = argparse.ArgumentParser(description='Train the Model on images and target masks',\n",
    "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    # parser.add_argument('-b', '--batch-size', metavar='B', type=int, nargs='?', default=2,\n",
    "    #                     help='Batch size', dest='batchsize')\n",
    "    parser.add_argument('-l', '--learning-rate', metavar='LR', type=float, nargs='?', default=0.001,\n",
    "                        help='Learning rate', dest='learning_rate')\n",
    "    parser.add_argument('-f', '--load', dest='load', type=str, default=None,\n",
    "                        help='Load model from a .pth file')\n",
    "    parser.add_argument('-g', '--gpu', metavar='G', type=str, default='-1',\n",
    "                        help='GPU', dest='gpu')\n",
    "    parser.add_argument('-dir', '--data-dir', type=str, default=None,\n",
    "                        help='dataset dir', dest='dataset_dir')\n",
    "    parser.add_argument('-pretrained', type=str, default=None, help='pretrained yolov4.conv.137')\n",
    "    parser.add_argument('-classes', type=int, default=80, help='dataset classes')\n",
    "    parser.add_argument('-train_label_path', dest='train_label', type=str, default='train.txt', help=\"train label path\")\n",
    "    parser.add_argument(\n",
    "        '-optimizer', type=str, default='adam',\n",
    "        help='training optimizer',\n",
    "        dest='TRAIN_OPTIMIZER')\n",
    "    parser.add_argument(\n",
    "        '-iou-type', type=str, default='iou',\n",
    "        help='iou type (iou, giou, diou, ciou)',\n",
    "        dest='iou_type')\n",
    "    parser.add_argument(\n",
    "        '-keep-checkpoint-max', type=int, default=10,\n",
    "        help='maximum number of checkpoints to keep. If set 0, all checkpoints will be kept',\n",
    "        dest='keep_checkpoint_max')\n",
    "    args = vars(parser.parse_args())\n",
    "\n",
    "    # for k in args.keys():\n",
    "    #     cfg[k] = args.get(k)\n",
    "    cfg.update(args)\n",
    "\n",
    "    return edict(cfg)\n",
    "\n",
    "def collate(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for img, box in batch:\n",
    "        images.append([img])\n",
    "        bboxes.append([box])\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    images = images.transpose(0, 3, 1, 2)\n",
    "    images = torch.from_numpy(images).div(255.0)\n",
    "    bboxes = np.concatenate(bboxes, axis=0)\n",
    "    bboxes = torch.from_numpy(bboxes)\n",
    "    return images, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid-dgx1/Hasnat/pytorch-YOLOv4/voc_images/\n",
      "/raid-dgx1/Hasnat/pytorch-YOLOv4/voc_images/\n"
     ]
    }
   ],
   "source": [
    "cfg = get_args(**Cfg)\n",
    "cfg.dataset_dir = '/raid-dgx1/Hasnat/pytorch-YOLOv4/voc_images/'\n",
    "config = cfg\n",
    "cfg.batch = 16\n",
    "train_dataset = Yolo_dataset(config.train_label, config, train=True)\n",
    "val_dataset = Yolo_dataset(config.val_label, config, train=False)\n",
    "\n",
    "n_train = len(train_dataset)\n",
    "n_val = len(val_dataset)\n",
    "\n",
    "print(cfg.dataset_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch // config.subdivisions, shuffle=True,\n",
    "                          num_workers=16, pin_memory=False, drop_last=True, collate_fn=collate)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch // config.subdivisions, shuffle=True, num_workers=8,\n",
    "                            pin_memory=True, drop_last=True, collate_fn=val_collate)\n",
    "print(cfg.dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid-dgx1/Hasnat/pytorch-YOLOv4/voc_images/\n",
      "89379579\n"
     ]
    }
   ],
   "source": [
    "print(cfg.dataset_dir)\n",
    "ds = val_loader.dataset\n",
    "for img_idx in range(len(ds)):\n",
    "    img, targets = ds[img_idx]\n",
    "    \n",
    "    print(targets[\"image_id\"].item())\n",
    "    break\n",
    "    #img, targets = ds[img_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in function convert_to_coco_api...\n",
      "/raid-dgx1/Hasnat/pytorch-YOLOv4/voc_images/ 002361.jpg\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1e7c63557905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtv_reference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_to_coco_api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_coco_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_fmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coco'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/raid-dgx1/Hasnat/pytorch-YOLOv4/tool/tv_reference/coco_utils.py\u001b[0m in \u001b[0;36mconvert_to_coco_api\u001b[0;34m(ds, bbox_fmt)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# targets = ds.get_annotations(img_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mimg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mimg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "from tool.tv_reference.coco_utils import convert_to_coco_api\n",
    "coco = convert_to_coco_api(train_loader.dataset, bbox_fmt='coco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, batch in train_loader:\n",
    "#    print(i, batch)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'007903.jpg 99,142,499,374,17 360,102,500,254,14 0,72,184,374,14 \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = open('data/val.txt').readlines()\n",
    "\n",
    "#\n",
    "all_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "all_int_id = []\n",
    "for t_ in val_dataset.truth.keys():\n",
    "    tiid = int(hashlib.sha1(t_.encode(\"utf-8\")).hexdigest(), 16) % (10 ** 10)\n",
    "    all_int_id.append(tiid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4952, 4952, 4952)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_int_id), len(val_dataset.truth.keys()), len(set(all_int_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ptenv2)",
   "language": "python",
   "name": "ptenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
